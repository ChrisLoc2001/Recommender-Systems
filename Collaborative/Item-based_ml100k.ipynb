{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12958830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, mean_absolute_error, root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce7038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_url = '../datasets/ml-100k/u.data'\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "ratings = pd.read_csv(ratings_url, sep='\\t', names=columns)\n",
    "ratings = ratings.drop(columns=('timestamp'))\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = ratings.pivot_table(index='user_id', columns='item_id', values='rating').fillna(0)\n",
    "rating_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42827f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "train_matrix = train_data.pivot_table(index='user_id', columns='item_id', values='rating').fillna(0)\n",
    "test_matrix = test_data.pivot_table(index='user_id', columns='item_id', values='rating').fillna(0)\n",
    "\n",
    "train_matrix = train_matrix.reindex(index=rating_matrix.index, columns=rating_matrix.columns, fill_value=0)\n",
    "test_matrix = test_matrix.reindex(index=rating_matrix.index, columns=rating_matrix.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_similarity = cosine_similarity(train_matrix.T)\n",
    "item_similarity = pd.DataFrame(item_similarity, index=rating_matrix.columns, columns=rating_matrix.columns)\n",
    "\n",
    "item_similarity.iloc[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fe24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_items(user_id, train_matrix, similarity_matrix, N=10):\n",
    "    # Rating dell'utente\n",
    "    user_ratings = train_matrix.loc[user_id]\n",
    "    # Score predetto per ogni item: somma weighted dei rating già espressi\n",
    "    scores = similarity_matrix.dot(user_ratings) / np.abs(similarity_matrix).sum(axis=1)\n",
    "    scores = pd.Series(scores, index=similarity_matrix.index)\n",
    "\n",
    "    # Escludiamo gli item già valutati\n",
    "    scores = scores[user_ratings == 0]\n",
    "    # Top-N raccomandazioni\n",
    "    top_n = scores.sort_values(ascending=False).head(N)\n",
    "    return top_n.index.tolist()\n",
    "\n",
    "\n",
    "recommend_items(1, train_matrix, item_similarity, N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = 100\n",
    "\n",
    "def evaluate(train_matrix, test_matrix, similarity_matrix, topN=10):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # Costruiamo vettori binari per ogni utente\n",
    "    for user in train_matrix.index:\n",
    "        # item realmente apprezzati nel test (rating >= 4)\n",
    "        true_items = set(test_matrix.columns[test_matrix.loc[user] >= 4])\n",
    "        # raccomandazioni\n",
    "        recs = set(recommend_items(user, train_matrix, similarity_matrix, N=topN))\n",
    "\n",
    "        # Calcolo TP, FP, FN, TN\n",
    "        tp = len(recs & true_items)\n",
    "        fp = len(recs - true_items)\n",
    "        fn = len(true_items - recs)\n",
    "        # per TN, consideriamo il resto degli item non raccomandati né nel test\n",
    "        all_items = set(train_matrix.columns)\n",
    "        non_recs = all_items - recs\n",
    "        tn = len(non_recs - true_items)\n",
    "\n",
    "        true_positives += tp\n",
    "        false_positives += fp\n",
    "        false_negatives += fn\n",
    "        true_negatives += tn\n",
    "\n",
    "        # MAE e RMSE: valutiamo i punteggi predetti vs. reali solo sugli item presenti nel test\n",
    "        user_ratings = train_matrix.loc[user]\n",
    "        scores = similarity_matrix.dot(user_ratings) / np.abs(similarity_matrix).sum(axis=1)\n",
    "        for item in test_matrix.columns:\n",
    "            if test_matrix.loc[user, item] > 0:\n",
    "                y_true.append(test_matrix.loc[user, item])\n",
    "                y_pred.append(scores[item])\n",
    "\n",
    "    # Metriche globali\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    \n",
    "    # Rimuovi valori NaN prima della valutazione\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    mask = ~np.isnan(y_pred)\n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return accuracy, precision, recall, mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17eda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, prec, rec, mae, rmse = evaluate(train_matrix, test_matrix, item_similarity, topN)\n",
    "print(f\"Accuracy: {acc:.4f}\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nMAE: {mae:.4f}\\nRMSE: {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
