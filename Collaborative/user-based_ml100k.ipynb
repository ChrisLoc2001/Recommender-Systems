{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77712f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_auc_score, root_mean_squared_error\n",
    "import numpy as np\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dati \n",
    "url = \"../datasets/ml-100k/u.data\"\n",
    "columns = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv(url, sep='\\t', names=columns)\n",
    "df.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione matrice utente-film\n",
    "ratings_matrix = df.pivot_table(index='user_id', columns='movie_id', values='rating')\n",
    "ratings_matrix.fillna(0, inplace=True)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_matrix = train_data.pivot_table(\n",
    "    index='user_id',\n",
    "    columns='movie_id',\n",
    "    values='rating'\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1818fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo similarità tra utenti\n",
    "user_similarity = cosine_similarity(train_matrix)\n",
    "user_similarity_df = pd.DataFrame(user_similarity, index=train_matrix.index, columns=train_matrix.index)\n",
    "\n",
    "# Funzione per raccomandare film a un utente basandosi sugli utenti simili\n",
    "def recommend_movies(user_id, num_recommendations=5):\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:]  # escludi se stesso\n",
    "    weighted_ratings = np.zeros(train_matrix.shape[1])\n",
    "    \n",
    "    for other_user, similarity in similar_users.items():\n",
    "        weighted_ratings += similarity * train_matrix.loc[other_user].values\n",
    "    \n",
    "    user_rated = train_matrix.loc[user_id].values > 0\n",
    "    weighted_ratings[user_rated] = 0  # Escludi quelli già visti\n",
    "    recommended_indices = np.argsort(weighted_ratings)[::-1][:num_recommendations]\n",
    "    \n",
    "    return train_matrix.columns[recommended_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione film raccomandati\n",
    "rec = recommend_movies(7)\n",
    "\n",
    "movies = pd.read_csv(\"../datasets/ml-100k/u.item\", sep=\"|\", encoding=\"latin-1\", \n",
    "                     names=[\"movie_id\", \"title\", \"release_date\", \"video_release_date\", \"IMDb_URL\", \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"])\n",
    "film = []\n",
    "for r in rec:\n",
    "    film.append(movies.loc[r, \"title\"])\n",
    "film"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868a08d",
   "metadata": {},
   "source": [
    "# Metriche di Valutazione su un sottoinsieme del test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848a748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costruzione predizioni binarie per valutazione\n",
    "threshold = 3 \n",
    "\n",
    "def predict_rating(user_id, movie_id):\n",
    "    if movie_id not in train_matrix.columns:\n",
    "        return 0\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:]\n",
    "    \n",
    "    num = 0\n",
    "    den = 0\n",
    "    for other_user, similarity in similar_users.items():\n",
    "        rating = train_matrix.loc[other_user, movie_id]\n",
    "        if rating > 0:\n",
    "            num += similarity * rating\n",
    "            den += similarity\n",
    "    if den == 0:\n",
    "        return 0\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ranked_list, ground_truth, k):\n",
    "    # Precision@k: fra i primi k, quanti sono rilevanti?\n",
    "    top_k = ranked_list[:k]\n",
    "    return len(set(top_k) & set(ground_truth)) / k\n",
    "\n",
    "def recall_at_k(ranked_list, ground_truth, k):\n",
    "    # Recall@k: quale frazione del ground_truth è nei primi k?\n",
    "    top_k = ranked_list[:k]\n",
    "    return len(set(top_k) & set(ground_truth)) / len(ground_truth)\n",
    "\n",
    "def ndcg_at_k(ranked_list, ground_truth, k):\n",
    "    # NDCG@k con labels binarie (1=relevant,0=non).\n",
    "    dcg = 0.0\n",
    "    idcg = sum(1.0 / np.log2(i+2) for i in range(min(len(ground_truth), k)))\n",
    "    for i, item in enumerate(ranked_list[:k]):\n",
    "        if item in ground_truth:\n",
    "            dcg += 1.0 / np.log2(i+2)\n",
    "    return dcg / idcg if idcg > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3.5\n",
    "K = 10   \n",
    "\n",
    "# Sampling test set\n",
    "test_sample = test_data.sample(1000, random_state=1)\n",
    "y_true_cont = []\n",
    "y_pred_cont = []\n",
    "y_true_bin = []\n",
    "y_pred_bin = []\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    u = row['user_id']\n",
    "    m = row['movie_id']\n",
    "    true_r = row['rating']\n",
    "    pred_r = predict_rating(u, m)\n",
    "    \n",
    "    y_true_cont.append(true_r)\n",
    "    y_pred_cont.append(pred_r)\n",
    "    \n",
    "    y_true_bin.append(int(true_r >= threshold))\n",
    "    y_pred_bin.append(int(pred_r >= threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d88125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating prediction metrics\n",
    "mae  = mean_absolute_error(y_true_cont, y_pred_cont)\n",
    "rmse = np.sqrt(mean_squared_error(y_true_cont, y_pred_cont))\n",
    "\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarized metrics\n",
    "accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
    "precision = precision_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "recall = recall_score(y_true_bin, y_pred_bin, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_true_bin, y_pred_cont) \n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"ROC‑AUC:   {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ae3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top‑N ranking metrics per utente\n",
    "from collections import defaultdict\n",
    "\n",
    "# raccogliamo vere interazioni positive nel test\n",
    "true_items = defaultdict(list)\n",
    "for _, row in test_sample.iterrows():\n",
    "    if row['rating'] >= threshold:\n",
    "        true_items[row['user_id']].append(row['movie_id'])\n",
    "\n",
    "# raccogliamo predizioni e ordiniamo\n",
    "pred_scores = defaultdict(list)\n",
    "for u in test_sample['user_id'].unique():\n",
    "    for m in test_sample['movie_id'].unique():\n",
    "        pred_scores[u].append((m, predict_rating(u, m)))\n",
    "    pred_scores[u].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# calcolo metriche aggregando per utente\n",
    "prec_k = []\n",
    "rec_k  = []\n",
    "ndcg_k = []\n",
    "\n",
    "for u, ranked in pred_scores.items():\n",
    "    ranked_list = [m for m, _ in ranked]\n",
    "    gt = true_items.get(u, [])\n",
    "    if not gt:\n",
    "        continue\n",
    "    prec_k.append(precision_at_k(ranked_list, gt, K))\n",
    "    rec_k.append(recall_at_k(ranked_list, gt, K))\n",
    "    ndcg_k.append(ndcg_at_k(ranked_list, gt, K))\n",
    "\n",
    "print(f\"Precision@{K}: {np.mean(prec_k):.4f}\")\n",
    "print(f\"Recall@{K}:    {np.mean(rec_k):.4f}\")\n",
    "print(f\"NDCG@{K}:      {np.mean(ndcg_k):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6668d0",
   "metadata": {},
   "source": [
    "# Metriche di valutazione su tutto il test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62815e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top‑N ranking metrics per utente - Su tutto il Test-split\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# raccogliamo vere interazioni positive nel test\n",
    "true_items = defaultdict(list)\n",
    "for _, row in test_data.iterrows():\n",
    "    if row['rating'] >= threshold:\n",
    "        true_items[row['user_id']].append(row['movie_id'])\n",
    "\n",
    "# raccogliamo predizioni e ordiniamo\n",
    "pred_scores = defaultdict(list)\n",
    "for u in test_data['user_id'].unique():\n",
    "    # considera tutti i movie_id o un sottoinsieme già visto\n",
    "    for m in test_data['movie_id'].unique():\n",
    "        pred_scores[u].append((m, predict_rating(u, m)))\n",
    "    # ordino decrescente\n",
    "    pred_scores[u].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# calcolo metriche aggregando per utente\n",
    "prec_k = []\n",
    "rec_k  = []\n",
    "ndcg_k = []\n",
    "\n",
    "for u, ranked in pred_scores.items():\n",
    "    ranked_list = [m for m, _ in ranked]\n",
    "    gt = true_items.get(u, [])\n",
    "    if not gt:\n",
    "        continue\n",
    "    prec_k.append(precision_at_k(ranked_list, gt, K))\n",
    "    rec_k.append(recall_at_k(ranked_list, gt, K))\n",
    "    ndcg_k.append(ndcg_at_k(ranked_list, gt, K))\n",
    "\n",
    "print(f\"Precision@{K}: {np.mean(prec_k):.4f}\")\n",
    "print(f\"Recall@{K}:    {np.mean(rec_k):.4f}\")\n",
    "print(f\"NDCG@{K}:      {np.mean(ndcg_k):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32145464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE E RMSE Su tutto il test split\n",
    "\n",
    "y_true = test_data['rating'].values\n",
    "y_pred = test_data.apply(lambda row: predict_rating(row['user_id'], row['movie_id']), axis=1).values\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valutazioni INIZIALI\n",
    "y_true_binary = []\n",
    "y_pred_binary = []\n",
    "y_true_continuous = []\n",
    "y_pred_continuous = []\n",
    "\n",
    "test_sample = test_data.sample(10000, random_state=1)\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    true_rating = row['rating']\n",
    "    pred_rating = predict_rating(row['user_id'], row['movie_id'])\n",
    "    \n",
    "    y_true_continuous.append(true_rating)\n",
    "    y_pred_continuous.append(pred_rating)\n",
    "    \n",
    "    actual = 1 if true_rating >= threshold else 0\n",
    "    predicted = 1 if pred_rating >= threshold else 0\n",
    "    \n",
    "    y_true_binary.append(actual)\n",
    "    y_pred_binary.append(predicted)\n",
    "\n",
    "# calcolo Accuracy, Precision, Recall\n",
    "accuracy = accuracy_score(y_true_binary, y_pred_binary)\n",
    "precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "\n",
    "# calcolo MAE e RMSE\n",
    "mae = mean_absolute_error(y_true_continuous, y_pred_continuous)\n",
    "rmse = root_mean_squared_error(y_true_continuous, y_pred_continuous))\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"MAE:       {mae:.4f}\")\n",
    "print(f\"RMSE:      {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
